{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5962 images belonging to 6 classes.\n",
      "Found 1490 images belonging to 6 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               163968    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,422,726\n",
      "Trainable params: 2,388,614\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "373/373 [==============================] - 63s 159ms/step - loss: 1.0972 - accuracy: 0.5270 - val_loss: 7.0512 - val_accuracy: 0.1081\n",
      "Epoch 2/10\n",
      "373/373 [==============================] - 60s 160ms/step - loss: 0.9215 - accuracy: 0.5971 - val_loss: 5.5792 - val_accuracy: 0.1772\n",
      "Epoch 3/10\n",
      "373/373 [==============================] - 58s 156ms/step - loss: 0.8626 - accuracy: 0.6276 - val_loss: 4.6380 - val_accuracy: 0.3040\n",
      "Epoch 4/10\n",
      "373/373 [==============================] - 60s 160ms/step - loss: 0.8154 - accuracy: 0.6672 - val_loss: 3.2726 - val_accuracy: 0.3376\n",
      "Epoch 5/10\n",
      "373/373 [==============================] - 56s 150ms/step - loss: 0.7982 - accuracy: 0.6667 - val_loss: 4.5927 - val_accuracy: 0.2597\n",
      "Epoch 6/10\n",
      "373/373 [==============================] - 59s 158ms/step - loss: 0.7669 - accuracy: 0.6793 - val_loss: 3.0170 - val_accuracy: 0.3651\n",
      "Epoch 7/10\n",
      "373/373 [==============================] - 62s 165ms/step - loss: 0.7494 - accuracy: 0.6895 - val_loss: 5.9033 - val_accuracy: 0.3275\n",
      "Epoch 8/10\n",
      "373/373 [==============================] - 60s 161ms/step - loss: 0.7358 - accuracy: 0.6998 - val_loss: 3.4160 - val_accuracy: 0.3268\n",
      "Epoch 9/10\n",
      "373/373 [==============================] - 56s 149ms/step - loss: 0.7180 - accuracy: 0.7127 - val_loss: 4.4980 - val_accuracy: 0.3383\n",
      "Epoch 10/10\n",
      "373/373 [==============================] - 59s 159ms/step - loss: 0.7010 - accuracy: 0.7137 - val_loss: 4.5542 - val_accuracy: 0.3362\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Define data generators for training and testing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split data into training and validation\n",
    ")\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create a single data generator that reads from 'train' directory and uses subfolder names as labels\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'skin/train',\n",
    "    target_size=(224, 224),  # Resize images to (224, 224)\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Assuming you have multiple classes (categorical classification)\n",
    "    subset='training'  # Use this for training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'skin/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use this for validation data\n",
    ")\n",
    "\n",
    "# Load MobileNetV2 Lite model (remove top classification layers)\n",
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom classification layers on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer with dynamic class count\n",
    "])\n",
    "\n",
    "# Change the learning rate to a different value, e.g., 0.001\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with the updated optimizer\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Define a model checkpoint to save the best model\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1644 images belonging to 6 classes.\n",
      "103/103 [==============================] - 5s 38ms/step - loss: 2.3850 - accuracy: 0.4234\n",
      "Test Accuracy: 42.34%\n"
     ]
    }
   ],
   "source": [
    "# Load the best saved model (the one with the highest validation accuracy)\n",
    "best_model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Create a test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)  # Normalize pixel values for test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'skin/test',\n",
    "    target_size=(224, 224),  # Resize images to (224, 224)\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Assuming you have multiple classes (categorical classification)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = best_model.evaluate(test_generator)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 525ms/step\n",
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Define a function to preprocess an image and make predictions\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(img)\n",
    "\n",
    "    # Get the class label with the highest probability\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return predicted_class[0]\n",
    "\n",
    "# Test the model on a new image\n",
    "image_path = 'download.jpeg'  # Replace with the path to your new image\n",
    "predicted_class = predict_image(image_path)\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5962 images belonging to 6 classes.\n",
      "Found 1490 images belonging to 6 classes.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inception_v3 (Functional)   (None, 8, 8, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 2048)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               262272    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,065,830\n",
      "Trainable params: 22,031,398\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "746/746 [==============================] - 158s 204ms/step - loss: 1.3379 - accuracy: 0.3839 - val_loss: 8.5228 - val_accuracy: 0.2148\n",
      "Epoch 2/15\n",
      "746/746 [==============================] - 152s 204ms/step - loss: 1.1408 - accuracy: 0.4557 - val_loss: 1.3225 - val_accuracy: 0.3933\n",
      "Epoch 3/15\n",
      "746/746 [==============================] - 155s 208ms/step - loss: 1.1151 - accuracy: 0.4767 - val_loss: 1.5423 - val_accuracy: 0.4309\n",
      "Epoch 4/15\n",
      "746/746 [==============================] - 157s 211ms/step - loss: 1.0542 - accuracy: 0.5081 - val_loss: 1.5601 - val_accuracy: 0.3906\n",
      "Epoch 5/15\n",
      "746/746 [==============================] - 159s 212ms/step - loss: 1.0150 - accuracy: 0.5248 - val_loss: 1.4730 - val_accuracy: 0.3638\n",
      "Epoch 6/15\n",
      "746/746 [==============================] - 157s 211ms/step - loss: 0.9958 - accuracy: 0.5339 - val_loss: 1.1433 - val_accuracy: 0.4852\n",
      "Epoch 7/15\n",
      "746/746 [==============================] - 159s 213ms/step - loss: 0.9830 - accuracy: 0.5377 - val_loss: 1.7292 - val_accuracy: 0.4087\n",
      "Epoch 8/15\n",
      "746/746 [==============================] - 165s 221ms/step - loss: 0.9672 - accuracy: 0.5584 - val_loss: 1.3813 - val_accuracy: 0.4275\n",
      "Epoch 9/15\n",
      "746/746 [==============================] - 166s 223ms/step - loss: 0.9695 - accuracy: 0.5580 - val_loss: 1.0838 - val_accuracy: 0.4893\n",
      "Epoch 10/15\n",
      "746/746 [==============================] - 163s 218ms/step - loss: 0.9451 - accuracy: 0.5716 - val_loss: 1.3902 - val_accuracy: 0.3463\n",
      "Epoch 11/15\n",
      "746/746 [==============================] - 158s 211ms/step - loss: 0.9381 - accuracy: 0.5746 - val_loss: 1.1869 - val_accuracy: 0.4919\n",
      "Epoch 12/15\n",
      "746/746 [==============================] - 159s 212ms/step - loss: 0.9186 - accuracy: 0.5761 - val_loss: 1.3087 - val_accuracy: 0.4570\n",
      "Epoch 13/15\n",
      "746/746 [==============================] - 162s 217ms/step - loss: 0.9058 - accuracy: 0.5884 - val_loss: 1.4416 - val_accuracy: 0.4282\n",
      "Epoch 14/15\n",
      "746/746 [==============================] - 159s 213ms/step - loss: 0.9028 - accuracy: 0.5872 - val_loss: 2.1122 - val_accuracy: 0.3248\n",
      "Epoch 15/15\n",
      "746/746 [==============================] - 158s 212ms/step - loss: 0.9050 - accuracy: 0.5907 - val_loss: 1.2040 - val_accuracy: 0.4718\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Set GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Define data generators for training and testing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split data into training and validation\n",
    ")\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "# Create a single data generator that reads from 'train' directory and uses subfolder names as labels\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'skin/train',\n",
    "    target_size=(299, 299),  # Resize images to (299, 299) for InceptionV3\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Assuming you have multiple classes (categorical classification)\n",
    "    subset='training'  # Use this for training data\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'skin/train',\n",
    "    target_size=(299, 299),  # Resize images to (299, 299) for InceptionV3\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use this for validation data\n",
    ")\n",
    "\n",
    "# Load InceptionV3 model with pre-trained weights (exclude top classification layers)\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Add custom classification layers on top\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer with dynamic class count\n",
    "])\n",
    "\n",
    "# Change the learning rate to a different value, e.g., 0.001\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with the updated optimizer\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochs = 15\n",
    "\n",
    "# Define a model checkpoint to save the best model\n",
    "checkpoint = ModelCheckpoint('best_inceptionv3_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1644 images belonging to 6 classes.\n",
      "103/103 [==============================] - 11s 94ms/step - loss: 1.2866 - accuracy: 0.4398\n",
      "Test Accuracy: 43.98%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_inceptionv3_model.h5')\n",
    "\n",
    "# Define data generator for testing\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)  # Normalize pixel values for test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'skin/test',\n",
    "    target_size=(299, 299),  # Resize images to (299, 299) for InceptionV3\n",
    "    batch_size=16,  # You can adjust the batch size as needed\n",
    "    class_mode='categorical',  # Assuming you have multiple classes (categorical classification)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 901ms/step\n",
      "Predicted Class: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('best_inceptionv3_model.h5')\n",
    "\n",
    "# Define a function to preprocess an image and make predictions\n",
    "def predict_image(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(299, 299))  # Resize to (299, 299) for InceptionV3\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(img)\n",
    "\n",
    "    # Get the class label with the highest probability\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return predicted_class[0]\n",
    "\n",
    "# Specify the path to the individual image you want to classify\n",
    "image_path = r'D:\\skindisease\\skin\\test\\Normal\\0_0_aidai_0084.jpg'  # Replace with the path to your single image\n",
    "predicted_class = predict_image(image_path)\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
